{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Siamese_network_forColab.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOcdcu/5uiZ/tBgitMmzZyO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"59KwljeBVS8D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"90a433dc-31cb-4d80-ccef-c52e5ccd0280","executionInfo":{"status":"ok","timestamp":1592164272634,"user_tz":-180,"elapsed":56934,"user":{"displayName":"Can Okan","photoUrl":"","userId":"17503674166406219642"}}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":2,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 144328 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.22-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.22-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.22-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7IFVoEU5VwIg","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E860pGqsVx13","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"c0d23505-de74-40ec-8e6a-82872ad20899","executionInfo":{"status":"ok","timestamp":1592167534377,"user_tz":-180,"elapsed":753,"user":{"displayName":"Can Okan","photoUrl":"","userId":"17503674166406219642"}}},"source":["import sys\n","sys.path.insert(0,'drive/signatures')\n","\n","\n","import numpy as np\n","import keras\n","from PIL import Image\n","import os \n","import glob\n","import cv2\n","from keras.preprocessing import image\n","import numpy as np\n","from natsort import natsorted, ns\n","import pandas as pd\n","import random\n","import cv2\n","\n","path_org='drive/signatures/full_org'\n","path_frog= 'drive/signatures/full_forg'\n","\n","\n","count_of_person=55\n","number_of_sample=24\n","\n","\n","def create_dataframe():\n","  fake_sign=[]\n","  for indis in range(1,56):#55 different classes   #56\n","      for j in range(1,number_of_sample+1): #25\n","          path=(path_frog+'/forgeries_'+str(indis)+'_'+str(j)+'.png')\n","          \n","            #img=cv2.imread(path)\n","           # p.append(path)\n","          fake_sign.append(path)\n","     \n","    \n","  real_sign=[]\n","  for indis in range(1,56):#55 different classes   #56\n","     for j in range(1,number_of_sample+1): #25\n","          \n","         path=(path_org+'/original_'+str(indis)+'_'+str(j)+'.png')\n","         #img=cv2.imread(path)\n","         #p.append(path)\n","         \n","         real_sign.append(path)  \n","     \n","    \n","    #her kisi icin 24 sample var\n","    #Sınıf saysı =55 \n","  \n","\n","  raw_data = {\"sign_1\":[], \"sign_2\":[], \"label\":[]}\n","  \n","  for kisi in range(count_of_person):\n"," \n","    real_signs_1=[]\n","    real_signs_2=[]\n","    fake_signs_1=[]\n","    \n","    indis_start = kisi*24\n","    indis_end = (kisi+1)*24\n","    \n","    for sample in range(indis_start,indis_end): \n","      real_signs_1.append(real_sign[sample])\n","      real_signs_2.append(real_sign[sample])\n","      raw_data[\"label\"].append(1) \n","      \n","      #etkiet 1 gerçek imza\n","      #label 1 represents the geniune pair\n","    #temp = (real_signs_1[-12:]+i1_batch_1[:-12])\n","    real_signs_1.extend(real_signs_2)\n","\n","    for sign in real_signs_2:\n","      fake_signs_1.append(sign)\n","    \n","    for j in range(indis_start,indis_end): \n","      fake_signs_1.append(fake_sign[j])\n","      raw_data[\"label\"].append(0)\n","       #etkiet 0 sahte imzaları temsil etmektedir\n","\n","    raw_data[\"sign_1\"].extend(real_signs_1) #real-real pairs\n","    raw_data[\"sign_2\"].extend(fake_signs_1) #fake-fake pairs\n","  df = pd.DataFrame(raw_data, columns = [\"sign_1\",\"sign_2\",\"label\"])\n","  return df\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","def train_val_dataset():\n","  data_frame = create_dataframe()\n","  print(data_frame.shape)\n","  \n","  data_frame=data_frame.reindex(np.random.permutation(data_frame.index))\n","  \n","  train_set, val_set = train_test_split(data_frame,test_size=0.3,random_state=0)\n","  \n","  return train_set, val_set\n","\n","train_set,val_set = train_val_dataset()\n","print(len(val_set))\n"],"execution_count":41,"outputs":[{"output_type":"stream","text":["(2640, 3)\n","792\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WfXvE6BhWynG","colab_type":"code","colab":{}},"source":["import cv2\n","import keras\n","\n","class SignatureSequence(keras.utils.Sequence):\n","    \n","    def __init__(self, df, batch_size, dim):\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.df = df\n","        self.labels = df[\"label\"]\n","        \n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        s_df=self.df.shape[0]\n","        n=np.floor(s_df/self.batch_size)\n","        return int(n)\n","\n","    def __getitem__(self, indis):\n","         #indexes\n","        batches = self.indises[indis*self.batch_size:(indis+1)*self.batch_size]\n","        items = [self.df.iloc[k] for k in batches]\n","        part1,part2 = self.generator(items)\n","        return part1,part2\n","\n","    def on_epoch_end(self):\n","        self.indises = np.arange(self.df.shape[0])\n","        np.random.shuffle(self.indises)\n","\n","    def generator(self, items):\n","        part_1 = np.empty((self.batch_size, *self.dim,1))#working with gray images\n","        part_2 = np.empty((self.batch_size, *self.dim,1))#working with gray images\n","        label = np.empty((self.batch_size), dtype=int)\n","        \n","        for i in range(len(items)):\n","            #image 1\n","            signature_1 = cv2.imread(items[i][\"sign_1\"])\n","           \n","            resized_signature = cv2.resize(signature_1,(220,155))\n","            gray_signature=cv2.cvtColor(resized_signature, cv2.COLOR_BGR2GRAY)\n","            ret,thr_img = cv2.threshold(gray_signature, 0, 255, cv2.THRESH_OTSU)\n","            normalized_signature=thr_img/255\n","            signature_expanded = normalized_signature[:, :, np.newaxis]\n","            signature_1=np.array(signature_expanded)\n","\n","            #image 2\n","            signature_2 = cv2.imread(items[i][\"sign_2\"])\n","            \n","            resized_signature = cv2.resize(signature_2,(220,155))\n","            gray_signature=cv2.cvtColor(resized_signature, cv2.COLOR_BGR2GRAY)\n","            ret,thr_img = cv2.threshold(gray_signature, 0, 255, cv2.THRESH_OTSU)\n","            normalized_signature=thr_img/255\n","            signature_expanded = normalized_signature[:, :, np.newaxis]\n","            signature_2=np.array(signature_expanded)\n","            \n","\n","            \n","          \n","            label[i] = items[i][\"label\"]\n","            part_1[i,] = signature_1\n","            part_2[i,] = signature_2 \n","            y=label\n","            x=[part_1 ,part_2]\n","\n","        return x, y\n","\n","\n","dim=(155,220)\n","batch_size=64\n","data_train = SignatureSequence(train_set,batch_size,dim)\n","data_validation = SignatureSequence(val_set,batch_size,dim)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqCHCPbVY-3B","colab_type":"code","colab":{}},"source":["########  Network ###############################\n","\n","from keras import models\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Activation, BatchNormalization, Input, Dropout, Flatten\n","from keras.models import Model\n","from keras.models import Sequential\n","\n","from keras.layers import Lambda\n","\n","\n","def network_model():\n","  input_shape=(155,220,1)\n","  in_imgLeft = Input(shape=input_shape, name=\"left_image\")\n","  in_imgRight = Input(shape=input_shape, name=\"right_image\")  \n","\n","  model = Sequential()\n","  #1st Conv layer\n","  model.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape))\n","  model.add( MaxPooling2D(pool_size=(3,3)) )\n","  #2nd Conv layer\n","  model.add( Conv2D(64, (3, 3), activation=\"relu\") )\n","  model.add( MaxPooling2D(pool_size=(3,3),strides=(2,2)) )\n","  \n","  #3rd Conv layer\n","  model.add( Conv2D(128, (3, 3), activation=\"relu\") )\n","  #4st Conv layer\n","  model.add( Conv2D(64, (3, 3), activation=\"relu\") )\n","  #5st Conv layer\n","  model.add( Conv2D(128, (3, 3), activation=\"relu\") )\n","\n","  model.add( MaxPooling2D(pool_size=(3,3),strides=(2,2)) )\n","\n","  model.add( Dropout(0.3) )\n","  #Fully Connected Layer\n","  model.add( Flatten() )\n","  \n","  model.add( Dense(256, activation=\"relu\") )\n","  model.add( Dropout(0.5) )\n","\n","  model.add( Dense(256, activation=\"relu\") )\n","\n","  \n","  left_branch = model(in_imgLeft)\n","  right_branch = model(in_imgRight)\n","\n","  cal = Lambda(euclidean_distance,output_shape=output_shape)\n","  distance=cal([left_branch, right_branch])\n","\n","  model = Model([in_imgLeft, in_imgRight], distance)\n","\n","\n","    \n","  return model\n","\n","\n","from keras import backend as K\n","\n","def euclidean_distance(vectors):\n","    xi, yi = vectors\n","    return K.sqrt(K.sum(K.square(xi - yi), axis=1, keepdims=True))\n","\n","def output_shape(shapes):\n","    shape1, shape2 = shapes\n","    return (shape1[0], 1)\n","\n","def contrastive_loss(l, y_pred):\n","    #l=label(y_true)\n","    margin = 1\n","    #http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n","    margin_square = (K.maximum(margin - y_pred, 0))**2\n","\n","    # α,β= ½ \n","    return K.mean((l * (y_pred)**2)*l + (1 - l)* margin_square)\n","  \n","  \n","def accuracy(y_true, y_pred):\n","    \n","    #y_true.dtype)\n","    casted_ytrue=K.cast(y_pred < 0.5, 'float32')\n","    return K.mean(K.equal(y_true,casted_ytrue ))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjHB6qHNZH4H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9a8d98ff-c60b-4b0f-8ebe-f5bd58a22f8d","executionInfo":{"status":"ok","timestamp":1592173065335,"user_tz":-180,"elapsed":1519408,"user":{"displayName":"Can Okan","photoUrl":"","userId":"17503674166406219642"}}},"source":["from keras.callbacks import ModelCheckpoint\n","\n","\n","model = network_model()\n","\n","optimizer = keras.optimizers.RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08)\n","model.compile(loss=contrastive_loss, optimizer=optimizer,metrics=[accuracy])\n","\n","\n","checkpointer = ModelCheckpoint(filepath=\"drive/best_weights_for_Network.hdf5\", \n","                               monitor = 'val_accuracy',\n","                               verbose=1, \n","                               save_best_only=True)\n","\n","history=model.fit_generator(generator=data_train,validation_data=data_validation, epochs=20,steps_per_epoch=56, validation_steps=24, use_multiprocessing=True, \n","                            workers=6,callbacks=[checkpointer])\n","\n","model.save(\"drive/siyam_RMSProp_model.h5\")\n"],"execution_count":59,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","56/56 [==============================] - 85s 2s/step - loss: 0.2492 - accuracy: 0.5745 - val_loss: 0.3149 - val_accuracy: 0.4993\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.49935, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 2/20\n","56/56 [==============================] - 71s 1s/step - loss: 0.2125 - accuracy: 0.6579 - val_loss: 0.2657 - val_accuracy: 0.5749\n","\n","Epoch 00002: val_accuracy improved from 0.49935 to 0.57487, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 3/20\n","56/56 [==============================] - 71s 1s/step - loss: 0.1958 - accuracy: 0.6864 - val_loss: 0.2763 - val_accuracy: 0.6009\n","\n","Epoch 00003: val_accuracy improved from 0.57487 to 0.60091, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 4/20\n","56/56 [==============================] - 71s 1s/step - loss: 0.1800 - accuracy: 0.7285 - val_loss: 0.2893 - val_accuracy: 0.6152\n","\n","Epoch 00004: val_accuracy improved from 0.60091 to 0.61523, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 5/20\n","56/56 [==============================] - 72s 1s/step - loss: 0.1682 - accuracy: 0.7433 - val_loss: 0.1609 - val_accuracy: 0.6654\n","\n","Epoch 00005: val_accuracy improved from 0.61523 to 0.66536, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 6/20\n","56/56 [==============================] - 73s 1s/step - loss: 0.1542 - accuracy: 0.7734 - val_loss: 0.1968 - val_accuracy: 0.6764\n","\n","Epoch 00006: val_accuracy improved from 0.66536 to 0.67643, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 7/20\n","56/56 [==============================] - 70s 1s/step - loss: 0.1423 - accuracy: 0.7952 - val_loss: 0.1755 - val_accuracy: 0.7181\n","\n","Epoch 00007: val_accuracy improved from 0.67643 to 0.71810, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 8/20\n","56/56 [==============================] - 72s 1s/step - loss: 0.1249 - accuracy: 0.8245 - val_loss: 0.2067 - val_accuracy: 0.7793\n","\n","Epoch 00008: val_accuracy improved from 0.71810 to 0.77930, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 9/20\n","56/56 [==============================] - 74s 1s/step - loss: 0.1137 - accuracy: 0.8491 - val_loss: 0.1891 - val_accuracy: 0.7409\n","\n","Epoch 00009: val_accuracy did not improve from 0.77930\n","Epoch 10/20\n","56/56 [==============================] - 74s 1s/step - loss: 0.1021 - accuracy: 0.8664 - val_loss: 0.1452 - val_accuracy: 0.7474\n","\n","Epoch 00010: val_accuracy did not improve from 0.77930\n","Epoch 11/20\n","56/56 [==============================] - 75s 1s/step - loss: 0.0899 - accuracy: 0.8803 - val_loss: 0.1361 - val_accuracy: 0.8079\n","\n","Epoch 00011: val_accuracy improved from 0.77930 to 0.80794, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 12/20\n","56/56 [==============================] - 73s 1s/step - loss: 0.0774 - accuracy: 0.9057 - val_loss: 0.1530 - val_accuracy: 0.7988\n","\n","Epoch 00012: val_accuracy did not improve from 0.80794\n","Epoch 13/20\n","56/56 [==============================] - 73s 1s/step - loss: 0.0686 - accuracy: 0.9177 - val_loss: 0.1322 - val_accuracy: 0.8424\n","\n","Epoch 00013: val_accuracy improved from 0.80794 to 0.84245, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 14/20\n","56/56 [==============================] - 77s 1s/step - loss: 0.0618 - accuracy: 0.9319 - val_loss: 0.0980 - val_accuracy: 0.8529\n","\n","Epoch 00014: val_accuracy improved from 0.84245 to 0.85286, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 15/20\n","56/56 [==============================] - 77s 1s/step - loss: 0.0531 - accuracy: 0.9434 - val_loss: 0.0974 - val_accuracy: 0.8724\n","\n","Epoch 00015: val_accuracy improved from 0.85286 to 0.87240, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 16/20\n","56/56 [==============================] - 73s 1s/step - loss: 0.0481 - accuracy: 0.9512 - val_loss: 0.0881 - val_accuracy: 0.8509\n","\n","Epoch 00016: val_accuracy did not improve from 0.87240\n","Epoch 17/20\n","56/56 [==============================] - 74s 1s/step - loss: 0.0438 - accuracy: 0.9534 - val_loss: 0.0756 - val_accuracy: 0.8711\n","\n","Epoch 00017: val_accuracy did not improve from 0.87240\n","Epoch 18/20\n","56/56 [==============================] - 74s 1s/step - loss: 0.0371 - accuracy: 0.9623 - val_loss: 0.0759 - val_accuracy: 0.8587\n","\n","Epoch 00018: val_accuracy did not improve from 0.87240\n","Epoch 19/20\n","56/56 [==============================] - 74s 1s/step - loss: 0.0322 - accuracy: 0.9651 - val_loss: 0.1078 - val_accuracy: 0.8861\n","\n","Epoch 00019: val_accuracy improved from 0.87240 to 0.88607, saving model to drive/best_weights_for_Network.hdf5\n","Epoch 20/20\n","56/56 [==============================] - 73s 1s/step - loss: 0.0284 - accuracy: 0.9710 - val_loss: 0.0544 - val_accuracy: 0.8952\n","\n","Epoch 00020: val_accuracy improved from 0.88607 to 0.89518, saving model to drive/best_weights_for_Network.hdf5\n"],"name":"stdout"}]}]}